{"cells":[{"cell_type":"markdown","metadata":{"id":"RlZu1rxMt77N"},"source":["# For Training and Loading the Pretrained Model on a Fresh Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkwITU5hFTpM"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import imread\n","from keras.layers import Input\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","# import tf.keras.callbacks "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1669135863556,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"ZyAYm1hIJ5AF","outputId":"994730e5-ad14-4af8-c736-38695f0fe91e"},"outputs":[{"output_type":"stream","name":"stdout","text":["TF version: 2.9.2\n","Hub version: 0.12.0\n","GPU available\n"]}],"source":["print(\"TF version:\", tf.__version__)\n","print(\"Hub version:\", hub.__version__)\n","\n","# Check for GPU\n","print(\"GPU\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"not available\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3758,"status":"ok","timestamp":1669139506280,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"9rcTFkleBhQb","outputId":"05e6a81d-df9a-42bf-e684-868bd8ff2148"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}],"source":["# # Running this cell will provide you with a token to link your drive to this notebook\n","from google.colab import drive\n","import sys\n","\n","drive.mount('/content/gdrive/')\n","sys.path.append('/content/gdrive/My Drive/Datathon/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1669139506280,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"wYX_zML-Bvwv","outputId":"8a26b77b-a163-4c78-946e-9272db58ec3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/18Pjuiby86W8tPsgJuQAMo0AMjzsG0pLw/Datathon\n"]}],"source":["%cd '/content/gdrive/My Drive/Datathon/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1669135867967,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"Ay8RDA8iB7lZ","outputId":"5e305808-b59e-44b3-96d8-ddf3c2364b05"},"outputs":[{"output_type":"stream","name":"stdout","text":["'~'\t\t\t\t        mobile_net_l.png\n"," Datathon2.ipynb\t\t        models\n"," datathonindoml-2022.zip\t        models_archit\n"," densenet_a.png\t\t\t       'New Models.ipynb'\n"," dense-net-logger.csv\t\t        predicted_label_3.csv\n"," densenet_l.png\t\t\t        predicted_label.csv\n"," drive\t\t\t\t       'rekhani LSTM.ipynb'\n"," efficient_net_a.png\t\t        resnet_a.png\n"," efficient-net-logger.csv\t        res-net-logger.csv\n"," efficient_net_l.png\t\t        resnet_l.png\n","'few shot.ipynb'\t\t        sample_submission.csv\n","'for RESNETS.ipynb'\t\t        train\n"," full_np_array.npy\t\t        train_labels.csv\n"," kaggle-indoml-submission.csv\t        validation\n"," kaggle-indoml-submission-model-2.csv   vision_trans_a.png\n"," logs\t\t\t\t       'Vision Transformers.ipynb'\n"," mobile_net_a.png\t\t        vision-trans-logger.csv\n"," mobile-net-logger.csv\t\t        vision_trans_l.png\n"," mobile-net-logger.gsheet\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tO79ce92J6hH"},"outputs":[],"source":["train_labels_csv = pd.read_csv(\"train_labels.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDXtkZfjKF-H"},"outputs":[],"source":["labels = train_labels_csv[\"label\"].to_numpy() # convert labels column to NumPy array (from Training Dataset)\n","# Finding the unique labels\n","unique_labels = np.unique(labels)\n","# Turn every label into a boolean array\n","boolean_labels = [label == np.array(unique_labels) for label in labels]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2yeVv3pMKVWM"},"outputs":[],"source":["# Create pathnames from image ID's\n","train_path = \"train/train/\"\n","filenames = [train_path + str(fname) + \".jpeg\" for fname in train_labels_csv[\"id\"]]      # Fetching training files' IDs from train_labels_csv\n","\n","val_path = \"validation/validation/\"\n","val_filenames = [val_path + str(fname) for fname in os.listdir(val_path)]       # Fetching Validation files' IDs from the validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wt7g4P4qNa2W"},"outputs":[],"source":["X_test = filenames[4000:5000]\n","y_test = boolean_labels[4000:5000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHQJsrvo5ndK"},"outputs":[],"source":["# Setup X & y variables\n","X = filenames[0:4000]\n","y = boolean_labels[0:4000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGGhJS5FKbVl"},"outputs":[],"source":["# Define image size\n","IMG_SIZE = 224\n","\n","def process_image(image_path):\n","  \"\"\"\n","  Takes an image file path and turns it into a Tensor.\n","  \"\"\"\n","  # Read in image file\n","  image = tf.io.read_file(image_path)\n","  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n","  image = tf.image.decode_jpeg(image, channels=3)\n","  # Convert the colour channel values from 0-225 values to 0-1 values\n","  image = tf.image.convert_image_dtype(image, tf.float32)\n","  # Resize the image to our desired size (224, 244)\n","  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n","  return image"]},{"cell_type":"code","source":["# Define image size\n","IMG_SIZE = 256\n","\n","def process_image_2(image_path):\n","  \"\"\"\n","  Takes an image file path and turns it into a Tensor.\n","  \"\"\"\n","  # Read in image file\n","  image = tf.io.read_file(image_path)\n","  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n","  image = tf.image.decode_jpeg(image, channels=3)\n","  # Convert the colour channel values from 0-225 values to 0-1 values\n","  image = tf.image.convert_image_dtype(image, tf.float32)\n","  # Resize the image to our desired size (224, 244)\n","  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n","  return image"],"metadata":{"id":"GxFWnCVsULgv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJEGS3j_Kk4N"},"outputs":[],"source":["# Create a simple function to return a tuple (image, label)\n","def get_image_label(image_path, label):\n","  \"\"\"\n","  Takes an image file path name and the associated label,\n","  processes the image and returns a tuple of (image, label).\n","  \"\"\"\n","  image = process_image(image_path)\n","  return image, label"]},{"cell_type":"code","source":["# Create a simple function to return a tuple (image, label)\n","def get_image_label_2(image_path, label):\n","  \"\"\"\n","  Takes an image file path name and the associated label,\n","  processes the image and returns a tuple of (image, label).\n","  \"\"\"\n","  image = process_image_2(image_path)\n","  return image, label"],"metadata":{"id":"UlmWAf8yUSi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hqzeHeIKmc9"},"outputs":[],"source":["# Define the batch size, 32 is a good default\n","BATCH_SIZE=32\n","# Create a function to turn data into batches\n","def create_data_batches(x, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\n","  \"\"\"\n","  Creates batches of data out of image (x) and label (y) pairs.\n","  Shuffles the data if it's training data but doesn't shuffle it if it's validation data.\n","  Also accepts test data as input (no labels).\n","  \"\"\"\n","  # If the data is a training dataset, we shuffle it\n","  print(\"Creating training data batches...\")\n","  # Turn filepaths and labels into Tensors\n","  data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths\n","                                            tf.constant(y))) # labels\n","  \n","\n","  # Create (image, label) tuples (this also turns the image path into a preprocessed image)\n","  data = data.map(get_image_label)\n","\n","    # Turn the data into batches\n","  data_batch = data.batch(BATCH_SIZE)\n","  return data_batch"]},{"cell_type":"code","source":["# Define the batch size, 32 is a good default\n","BATCH_SIZE=32\n","# Create a function to turn data into batches\n","def create_data_batches_2(x, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\n","  \"\"\"\n","  Creates batches of data out of image (x) and label (y) pairs.\n","  Shuffles the data if it's training data but doesn't shuffle it if it's validation data.\n","  Also accepts test data as input (no labels).\n","  \"\"\"\n","  # If the data is a training dataset, we shuffle it\n","  print(\"Creating training data batches...\")\n","  # Turn filepaths and labels into Tensors\n","  data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths\n","                                            tf.constant(y))) # labels\n","  \n","\n","  # Create (image, label) tuples (this also turns the image path into a preprocessed image)\n","  data = data.map(get_image_label_2)\n","\n","    # Turn the data into batches\n","  data_batch = data.batch(BATCH_SIZE)\n","  return data_batch"],"metadata":{"id":"45Bx7SuVUYXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1669135889044,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"i_EFOCFt1-6q","outputId":"da1b99c8-13fe-4ff0-823f-1c33f43785df"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4000"]},"metadata":{},"execution_count":61}],"source":["len(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E95RHTk_Kn3d"},"outputs":[],"source":["# Turn full training data in a data batch\n","\n","# full_data = create_data_batches(X, y, 4000)"]},{"cell_type":"code","source":["full_data_test = create_data_batches(X_test, y_test, 32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FpEm-Mh_LYj8","executionInfo":{"status":"ok","timestamp":1669136347364,"user_tz":-330,"elapsed":520,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"92d2bdd8-61aa-499d-b315-98039f1a8d34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating training data batches...\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":479,"status":"ok","timestamp":1669136356808,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"DhhLA3xoKyuU","outputId":"410bbd9a-9095-44ed-c744-e20ac5cd3a56"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 16), dtype=tf.bool, name=None))>"]},"metadata":{},"execution_count":86}],"source":["full_data_test"]},{"cell_type":"code","source":["full_data_test_2= create_data_batches(X_test, y_test, 32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cEY0Mr89UgdS","executionInfo":{"status":"ok","timestamp":1669137052568,"user_tz":-330,"elapsed":6,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"6c8bacdd-0d78-4182-9673-6f328f545e0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating training data batches...\n"]}]},{"cell_type":"code","source":["full_data_test_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOtwTYbgUjw8","executionInfo":{"status":"ok","timestamp":1669137060519,"user_tz":-330,"elapsed":14,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"c7ed11a6-05b8-4f05-f884-00b2a7adc7a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 16), dtype=tf.bool, name=None))>"]},"metadata":{},"execution_count":94}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JQSjm4q_1Et"},"outputs":[],"source":["# for image, label in full_data.map:\n","#   print(image.shape, label.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4G6G1QOKo6N"},"outputs":[],"source":["# Setup input shape to the model\n","INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch, height, width, colour channels\n","\n","# Setup output shape of the model\n","OUTPUT_SHAPE = len(unique_labels) # number of unique labels\n","\n","# Setup model URL from TensorFlow Hub\n","# MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5\"\n","# MODEL_URL =\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/classification/2\"\n","MODEL_URL =\"https://tfhub.dev/google/supcon/resnet_v1_200/imagenet/classification/1\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYIutERlKrIH"},"outputs":[],"source":["# we will build the model using the Keras API\n","\n","def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):\n","  print(\"Building the model with:\", MODEL_URL)\n","\n","  # Setup the model layers\n","  model = tf.keras.Sequential([\n","    hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)\n","    tf.keras.layers.Dense(units=OUTPUT_SHAPE, \n","                          activation=\"softmax\") # Layer 2 (output layer). Softmax will predict the probabilities for each class for each image\n","  ])\n","\n","  # Compile the model\n","  model.compile(\n","      loss=tf.keras.losses.CategoricalCrossentropy(), # Our model wants to reduce this (how wrong its guesses are)\n","      optimizer=tf.keras.optimizers.Adam(), # An optimizer helping our model how to improve its guesses\n","      metrics=[\"accuracy\"] # We'd like this to go up\n","  )\n","\n","  # Build the model\n","  model.build(INPUT_SHAPE) # Let the model know what kind of inputs it'll be getting\n","  \n","  return model"]},{"cell_type":"markdown","metadata":{"id":"TQ5fZFDQ4gw5"},"source":["## Ensemble of top models by loss and Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5HVZ9qF5MrvZ"},"outputs":[],"source":["def save_model(model, suffix=None):\n","  \"\"\"\n","  Saves a given model in a models directory and appends a suffix (str)\n","  for clarity and reuse.\n","  \"\"\"\n","  # Create model directory with current time\n","  modeldir = os.path.join(\"models_archit\",\n","                          datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\"))\n","  model_path = modeldir + \"-\" + suffix + \".h5\" # save format of model\n","  print(f\"Saving model to: {model_path}...\")\n","  model.save(model_path)\n","  return model_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8u-65LpMt0w"},"outputs":[],"source":["def load_model(model_path):\n","  \"\"\"\n","  Loads a saved model from a specified path.\n","  \"\"\"\n","  print(f\"Loading saved model from: {model_path}\")\n","  model = tf.keras.models.load_model(model_path,\n","                                     custom_objects={\"KerasLayer\":hub.KerasLayer})\n","  return model"]},{"cell_type":"code","source":["full_data_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5ePPG1jQ6nk","executionInfo":{"status":"ok","timestamp":1669136106437,"user_tz":-330,"elapsed":523,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"862034b5-4ece-40e4-c3f7-2455712d5382"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<MapDataset element_spec=(TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(16,), dtype=tf.bool, name=None))>"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEKU-VFkdko2","executionInfo":{"status":"ok","timestamp":1669139527808,"user_tz":-330,"elapsed":449,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"1699299f-919f-4498-f823-7514ad97c255"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'~'\t\t\t\t        mobile_net_l.png\n"," Datathon2.ipynb\t\t        models\n"," datathonindoml-2022.zip\t        models_archit\n"," densenet_a.png\t\t\t       'New Models.ipynb'\n"," dense-net-logger.csv\t\t        predicted_label_3.csv\n"," densenet_l.png\t\t\t        predicted_label.csv\n"," drive\t\t\t\t       'rekhani LSTM.ipynb'\n"," efficient_net_a.png\t\t        resnet_a.png\n"," efficient-net-logger.csv\t        res-net-logger.csv\n"," efficient_net_l.png\t\t        resnet_l.png\n","'few shot.ipynb'\t\t        sample_submission.csv\n","'for RESNETS.ipynb'\t\t        train\n"," full_np_array.npy\t\t        train_labels.csv\n"," kaggle-indoml-submission.csv\t        validation\n"," kaggle-indoml-submission-model-2.csv   vision_trans_a.png\n"," logs\t\t\t\t       'Vision Transformers.ipynb'\n"," mobile_net_a.png\t\t        vision-trans-logger.csv\n"," mobile-net-logger.csv\t\t        vision_trans_l.png\n"," mobile-net-logger.gsheet\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgffU6nG94Mm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669140516488,"user_tz":-330,"elapsed":130781,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"a716b063-c2ab-4e29-e7c8-76e39d8b8059"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading saved model from: models_archit/20221112-15151668266108-res-net.h5\n","32/32 [==============================] - 15s 341ms/step - loss: 1.7276 - accuracy: 0.4530\n","Loading saved model from: models_archit/20221112-09411668246088-dense-net.h5\n","32/32 [==============================] - 8s 230ms/step - loss: 4.5251 - accuracy: 0.2220\n","Loading saved model from: models_archit/20221112-00031668211416-mobile-net.h5\n","32/32 [==============================] - 10s 284ms/step - loss: 1.2096 - accuracy: 0.6310\n","Loading saved model from: models/20221007-04581665118720-initial-4000-images-Adam.h5\n","32/32 [==============================] - 8s 230ms/step - loss: 1.5378 - accuracy: 0.5470\n","Loading saved model from: models/20221007-08281665131319-full-trained-adam.h5\n","32/32 [==============================] - 7s 216ms/step - loss: 0.9350 - accuracy: 0.7040\n","Loading saved model from: models/20221008-14551665240924-full-model-2-Adam.h5\n","32/32 [==============================] - 7s 209ms/step - loss: 0.8910 - accuracy: 0.7200\n","32/32 [==============================] - 7s 214ms/step\n","Loading saved model from: models_archit/20221112-02251668219959-efficient-net.h5\n","32/32 [==============================] - 7s 207ms/step - loss: 1.2357 - accuracy: 0.6060\n","32/32 [==============================] - 8s 219ms/step\n"]}],"source":["# # Evaluate the loaded model for all models\n","\n","# resnet_archit\n","loaded_model = load_model('models_archit/20221112-15151668266108-res-net.h5')\n","loaded_model.evaluate(full_data_test)\n","\n","# dense\n","loaded_model = load_model('models_archit/20221112-09411668246088-dense-net.h5')\n","loaded_model.evaluate(full_data_test)\n","\n","# mobile\n","loaded_model = load_model('models_archit/20221112-00031668211416-mobile-net.h5')\n","loaded_model.evaluate(full_data_test)\n","\n","# mobile_raj\n","loaded_model = load_model('models/20221007-04581665118720-initial-4000-images-Adam.h5')\n","loaded_model.evaluate(full_data_test)\n","\n","# mobile_raj_again\n","loaded_model = load_model('models/20221007-08281665131319-full-trained-adam.h5')\n","loaded_model.evaluate(full_data_test)\n","\n","# mobile_raj_again_again\n","loaded_model = load_model('models/20221008-14551665240924-full-model-2-Adam.h5')\n","loaded_model.evaluate(full_data_test)\n","predictions = loaded_model.predict(full_data_test, verbose=1) \n","predictions=pd.DataFrame(predictions)\n","predictions.to_csv('final_submit_approx_predicted_label_mobile-net-full.csv')\n","\n","# efficient\n","loaded_model = load_model('models_archit/20221112-02251668219959-efficient-net.h5')\n","loaded_model.evaluate(full_data_test)\n","predictions = loaded_model.predict(full_data_test, verbose=1) \n","predictions=pd.DataFrame(predictions)\n","predictions.to_csv('final_submit_approx_predicted_label_efficient-net.csv')\n"]},{"cell_type":"code","source":["# trans\n","loaded_model = load_model('models_archit/20221112-20131668284006-vision-trans.h5')\n","loaded_model.evaluate(full_data_test_2)\n","\n","predictions = loaded_model.predict(full_data_test_2, verbose=1) \n","predictions=pd.DataFrame(predictions)\n","predictions.to_csv('final_submit_approx_predicted_label_vision_trans.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6bKIb8HTg6Q","executionInfo":{"status":"ok","timestamp":1669140545782,"user_tz":-330,"elapsed":29309,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"bfe6a367-f820-4668-f155-2e42487369c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading saved model from: models_archit/20221112-20131668284006-vision-trans.h5\n","32/32 [==============================] - 9s 237ms/step - loss: 1.5112 - accuracy: 0.7520\n","32/32 [==============================] - 9s 239ms/step\n"]}]},{"cell_type":"code","source":["\n","p1=pd.read_csv('final_submit_approx_predicted_label_efficient-net.csv').drop(['Unnamed: 0'],axis=1)\n","p2=pd.read_csv('final_submit_approx_predicted_label_mobile-net-full.csv').drop(['Unnamed: 0'],axis=1)\n","p3=pd.read_csv('final_submit_approx_predicted_label_vision_trans.csv').drop(['Unnamed: 0'],axis=1)\n"],"metadata":{"id":"2rOL-I2lfbMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p_loss=p1*0.6060+p2*0.7200+p3*0.7520/(0.6060+0.7200+0.7520)\n","p_acc=(p1*(1/1.2357)+p2*(1/0.8910)+p3*(1/1.5112))/((1/1.2357)+(1/0.8910)+(1/1.5112))"],"metadata":{"id":"CfIDBPq1iD1u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p_loss\n","p_loss.to_csv('ensemble_loss.csv')"],"metadata":{"id":"BHPBuZKylLIQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss=tf.keras.losses.CategoricalCrossentropy()"],"metadata":{"id":"vnAZsSQZl5cy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p_acc\n","p_acc.to_csv('ensemble_accuracy.csv')"],"metadata":{"id":"0eB9NxEClNXq"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}