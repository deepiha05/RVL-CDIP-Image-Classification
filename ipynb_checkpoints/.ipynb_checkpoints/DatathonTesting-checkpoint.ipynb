{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvSmPxOcu_9V"
   },
   "source": [
    "# For Loading and Testing the Pretrained Model on a Fresh Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k61YQxUmvFR3"
   },
   "outputs": [],
   "source": [
    "# The '-d' parameter shows the destination for where the files should go\n",
    "!unzip \"drive/MyDrive/Datathon/datathonindoml-2022.zip\" -d \"drive/MyDrive/Datathon/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaW3cR3lvJ8L"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.pyplot import imread\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GufvOriWvMY-"
   },
   "source": [
    "## Accessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oyn1m0zlvT3r"
   },
   "outputs": [],
   "source": [
    "train_labels_csv = pd.read_csv(\"drive/MyDrive/Datathon/train_labels.csv\")\n",
    "\n",
    "# convert labels column to NumPy array (from Training Dataset)\n",
    "labels = train_labels_csv[\"label\"].to_numpy() \n",
    "# Finding the unique labels\n",
    "unique_labels = np.unique(labels)\n",
    "# Turn every label into a boolean array\n",
    "boolean_labels = [label == np.array(unique_labels) for label in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3csSIQAvbgY"
   },
   "outputs": [],
   "source": [
    "# Create pathnames from image ID's\n",
    "train_path = \"drive/MyDrive/Datathon/train/train/\"\n",
    "filenames = [train_path + str(fname) + \".tif\" for fname in train_labels_csv[\"id\"]]      # Fetching training files' IDs from train_labels_csv\n",
    "\n",
    "val_path = \"drive/MyDrive/Datathon/validation/validation/\"\n",
    "val_filenames = [val_path + str(fname) for fname in os.listdir(val_path)]       # Fetching Validation files' IDs from the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeHo64HSvibB"
   },
   "source": [
    "## Preprocessing the Images (Turning images into Tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtz4pjqjv83O"
   },
   "source": [
    "Running the next cell may take a significant amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAoiOvArvx9v"
   },
   "outputs": [],
   "source": [
    "# Renaming the training filenames from (.tif) to (.jpg) in order to convert them into tensors later on\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "  os.rename(filenames[i], filenames[i].split(\".\")[0] + '.jpeg') \n",
    "\n",
    "# Renaming the validation filenames from (.tif) to (.jpg) in order to convert them into tensors later on\n",
    "\n",
    "for i in range(len(val_filenames)):\n",
    "  os.rename(val_filenames[i], val_filenames[i].split(\".\")[0] + '.jpeg')     #rename the files (.tif) into (.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnKv2vztwBoN"
   },
   "source": [
    "NOTE: Running the next cell may take a significant amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqOwcv1WwFJv"
   },
   "outputs": [],
   "source": [
    "# Create pathnames from image ID's\n",
    "train_path = \"drive/MyDrive/Datathon/train/train/\"\n",
    "filenames = [train_path + str(fname) for fname in os.listdir(train_path)]     # Fetching training files' IDs from train_labels_csv\n",
    "\n",
    "# Converting all images into RGB Format in order to turn it into tensors\n",
    "\n",
    "for infile in filenames:\n",
    "    outfile = infile\n",
    "    im = Image.open(infile)\n",
    "    out = im.convert(\"RGB\")\n",
    "    out.save(outfile, \"JPEG\", quality=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BimKntCEwLQi"
   },
   "source": [
    "NOTE: Running the next cell may take a significant amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeeqQJA8wIaC"
   },
   "outputs": [],
   "source": [
    "val_path = \"drive/MyDrive/Datathon/validation/validation/\"\n",
    "val_filenames = [val_path + str(fname) for fname in os.listdir(val_path)]\n",
    "\n",
    "# Converting all images into RGB Format in order to turn it into tensors\n",
    "for infile in val_filenames:\n",
    "    outfile = infile\n",
    "    im = Image.open(infile)\n",
    "    out = im.convert(\"RGB\")\n",
    "    out.save(outfile, \"JPEG\", quality=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSZ2v_-QwdES"
   },
   "source": [
    "## Testing the Presaved Model on the converted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nIdYvogwlja"
   },
   "outputs": [],
   "source": [
    "# Define image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "def process_image(image_path):\n",
    "  \"\"\"\n",
    "  Takes an image file path and turns it into a Tensor.\n",
    "  \"\"\"\n",
    "  # Read in image file\n",
    "  image = tf.io.read_file(image_path)\n",
    "  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  # Convert the colour channel values from 0-225 values to 0-1 values\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "  # Resize the image to our desired size (224, 244)\n",
    "  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jJ1-FJ1wpta"
   },
   "outputs": [],
   "source": [
    "# Create a simple function to return a tuple (image, label)\n",
    "def get_image_label(image_path, label):\n",
    "  \"\"\"\n",
    "  Takes an image file path name and the associated label,\n",
    "  processes the image and returns a tuple of (image, label).\n",
    "  \"\"\"\n",
    "  image = process_image(image_path)\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svq5L8poyVB8"
   },
   "outputs": [],
   "source": [
    "# Turn prediction probabilities into their labels (Document Types)\n",
    "def get_pred_label(prediction_probabilities):\n",
    "  \"\"\"\n",
    "  Turns an array of prediction probabilities into a label.\n",
    "  \"\"\"\n",
    "  return unique_labels[np.argmax(prediction_probabilities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqhBvKF8wsqq"
   },
   "outputs": [],
   "source": [
    "# Define the batch size, 32 is a good default\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create a function to turn data into batches\n",
    "def create_data_batches(x, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\n",
    "  \"\"\"\n",
    "  Creates batches of data out of image (x) and label (y) pairs.\n",
    "  Shuffles the data if it's training data but doesn't shuffle it if it's validation data.\n",
    "  Also accepts test data as input (no labels).\n",
    "  \"\"\"\n",
    "  # If the data is a test dataset, we probably don't have labels\n",
    "  if test_data:\n",
    "    print(\"Creating test data batches...\")\n",
    "    data = tf.data.Dataset.from_tensor_slices((tf.constant(x))) # only filepaths\n",
    "    data_batch = data.map(process_image).batch(BATCH_SIZE)\n",
    "    return data_batch\n",
    "  \n",
    "  # If the data if a valid dataset, we don't need to shuffle it\n",
    "  elif valid_data:\n",
    "    print(\"Creating validation data batches...\")\n",
    "    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths\n",
    "                                               tf.constant(y))) # labels\n",
    "    data_batch = data.map(get_image_label).batch(BATCH_SIZE)\n",
    "    return data_batch\n",
    "\n",
    "  else:\n",
    "    # If the data is a training dataset, we shuffle it\n",
    "    print(\"Creating training data batches...\")\n",
    "    # Turn filepaths and labels into Tensors\n",
    "    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths\n",
    "                                              tf.constant(y))) # labels\n",
    "    \n",
    "\n",
    "    # Create (image, label) tuples (this also turns the image path into a preprocessed image)\n",
    "    data = data.map(get_image_label)\n",
    "\n",
    "    # Turn the data into batches\n",
    "    data_batch = data.batch(BATCH_SIZE)\n",
    "  return data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4sV5MWiwwSa"
   },
   "outputs": [],
   "source": [
    "# Setup input shape to the model\n",
    "INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch, height, width, colour channels\n",
    "\n",
    "# Setup output shape of the model\n",
    "OUTPUT_SHAPE = len(unique_labels) # number of unique labels\n",
    "\n",
    "# Setup model URL from TensorFlow Hub\n",
    "MODEL_URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\"\n",
    "MODEL_URL_2 = \"https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/classification/5\"\n",
    "MODEL_URL_3 = \"https://tfhub.dev/google/imagenet/mobilenet_v1_025_224/classification/5\"\n",
    "MODEL_URL_4 = \"https://tfhub.dev/google/imagenet/mobilenet_v2_075_224/classification/5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHlA1izgwyIh"
   },
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "  \"\"\"\n",
    "  Loads a saved model from a specified path.\n",
    "  \"\"\"\n",
    "  print(f\"Loading saved model from: {model_path}\")\n",
    "  model = tf.keras.models.load_model(model_path,\n",
    "                                     custom_objects={\"KerasLayer\":hub.KerasLayer})\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYIGx_7ixKAi"
   },
   "source": [
    "## Final Testing using test() method as mentioned in the Datathon Submission Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2AH_gJZxAWC"
   },
   "outputs": [],
   "source": [
    "model_path = \"drive/MyDrive/Datathon/models/20221007-08281665131319-full-trained-adam.h5\" \n",
    "data_path = \"drive/MyDrive/Datathon/validation/validation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMWiCXUvxGl-"
   },
   "outputs": [],
   "source": [
    "# Function to test the validation data stored in 'data_path' with the model stored in 'model_path'\n",
    "# here, model_path = \"drive/MyDrive/Datathon/models/20221007-08281665131319-full-trained-adam.h5\" \n",
    "#       data_path = \"drive/MyDrive/Datathon/validation/validation\"\n",
    "\n",
    "def test(model_path, data_path):\n",
    "  # Load the fully trained model\n",
    "  loaded_full_model = load_model(model_path)\n",
    "\n",
    "  # Load validation image filenames\n",
    "  val_path = data_path\n",
    "  val_filenames = [val_path + fname for fname in os.listdir(val_path)]\n",
    "\n",
    "  # Getting the list of validation set IDs\n",
    "  val_id = [id for id in os.listdir(val_path)]\n",
    "  val_ids = []\n",
    "  for item in val_id:\n",
    "    val_ids.append(int(item.split(\".\")[0]))\n",
    "  \n",
    "  # Create validation data batch so as to turn it into tensors and then fit it in our model\n",
    "  val_data = create_data_batches(val_filenames, test_data=True) \n",
    "\n",
    "  # Make predictions on the validation data \n",
    "  predictions = loaded_full_model.predict(val_data, verbose=1) \n",
    "  \n",
    "  # Getting the predicted labels in array val_pred_labels[]\n",
    "  val_pred_labels = []\n",
    "  for i in range(len(val_ids)):\n",
    "    val_pred_labels.append(get_pred_label(predictions[i]))\n",
    "  \n",
    "  # Fitting the data into Pandas dataframe\n",
    "  data = []\n",
    "  for i in range(len(val_ids)):\n",
    "    data.append((val_ids[i], val_pred_labels[i]))\n",
    "  df = pd.DataFrame(data, columns=['id','label'])\n",
    "\n",
    "  # Saving the predicted labels on validation set images in CSV\n",
    "  # Saving the predictions to predicted_label.csv file and saving it inside the datathon folder in GDrive\n",
    "  # df.to_csv(r'drive/MyDrive/Datathon/predicted_label2.csv', index=False) \n",
    "  df.to_csv(r'drive/MyDrive/Datathon/predicted_label.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123784,
     "status": "ok",
     "timestamp": 1665320939196,
     "user": {
      "displayName": "Raj Shekhar Vaghela",
      "userId": "17386515312137102891"
     },
     "user_tz": -330
    },
    "id": "jLgInmQCyBXw",
    "outputId": "a60ab417-3fbc-4479-97f8-c330a67668d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model from: drive/MyDrive/Datathon/models/20221007-08281665131319-full-trained-adam.h5\n",
      "Creating test data batches...\n",
      "29/29 [==============================] - 44s 1s/step\n"
     ]
    }
   ],
   "source": [
    "test(model_path, data_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
